\section{Model Database Design Principles and Tools}
\label{sec:design}
A data warehouse designer uses well understood techniques to choose which added-value tables to materialize in the warehouse in order to support a .%
\footnote{Such tables are often called {\em materialized views}.}
In a sense, Plato is also a data warehouse, where models provide added value on the measurements data, by revealing the real world reality behind the measurements. Similarly, a Plato warehouse designer will need to make a good choice of tables and models, based on appropriate criteria, whose application can be assisted by tools. 

The first one, called {\em precomputation} design criterion, is similar to conventional warehouse design. It calls for computing in advance the models that are used in recurrent queries, instead of computing the models from the measurements online. The latter approach would be too slow.

The second one, which we call {\em consolidation}, is novel and enables higher compression of the representations. In a sense, it is a probabilistic extension of database normalization, dictating that if some models are heavily correlated, the administrator should consider consolidating them into a single model, even if this single model is not being used directly by a query. \reminder{I would like a better name for ``consolidation"}

\begin{example}
\label{xmpl:design-choices}
For instance, Example~\ref{xmpl:models-and-definitions} models the sensor data by a table \texttt{sensor\_models(sensor, temp\_model)}, i.e., each sensor has a corresponding temporal model. The next example models the sensor data by a single model \texttt{full\_model} that predicts the temperature based on \texttt{x}, \texttt{y} and \texttt{t}. Next, let us assume that the application often issues queries that ask for the predicted temperatures at locations other than the locations of the sensors. While it is possible for the query to compute the required predictions online from the models of  \texttt{sensor\_models(sensor, temp\_model)}, such approach would probably be very slow. The precomputation design criterion says that, given such query workload, the model administrator must choose the single model.

Then let us consider an alternate scenario where the application does {\em not} issue queries that ask for predicted temperatures at arbitrary locations. Rather, it cares exclusively about the temperatures at the sensor sites. In such case the precomputation criterion points towards the ``many models" design of Example~\ref{xmpl:models-and-definitions}. Yet the consolidation criterion may still dictate that it is best to choose the single model of Example~\ref{xmpl:singlemodel}. The intuition is that the temperature sensors may be correlated since physics says that temperatures are correlated in sufficiently short distances. The consolidation criterion can be formally judged by comparing the entropy of the models of the ``many models" design against the entropy of the ``single model' design.
\end{example}


\reminder{Database normalization was misrepresented. It is more nuanced than the simplification of the example. I now question the usefulness of fully drawing out in the example the parallel with normalization (it will waste lines to make the solid exposition). I can still do it in abstract text. However, I suggest that our entropy presentation follows on the ``many models" Vs "single model" pattern set by the example. Do not worry about the temperature part. We will change it to some pollutant or something sexier.
}

\eat{
Suppose we have a relation with four attributes $A,B,C,D$. In the
un-normalized form we store the relation in a single table with four
columns. If there are mappings $B \to A$ and $B \to (C,D)$ then we can
use $B$ as a key and break the table into two smaller tables: one for
$A,B$ and one for $B,C,D$. 

To view the original table as a probability distribution, consider the
$A,B,C,D$ to be random variables with respect to some distribution
over the rows of the table (the uniform distribution is most commonly
used). We can now consider statistical dependenies between
attributes. For example the implication conditions described above
imply the following statistical conditions
\[
\left[ B=b \to A=a\right] \Rightarrow \left[ P(A=a | B=b) =1 \right]
\mbox{ and }
\left[ B=b \to (C,D)=(c,d) \right] \Rightarrow \left[ P(C=c,D=d | B=b) =1 \right]
\]
This gives a sufficient condition for compressibility, but it is far
from necessary. The value of the common variable $B$ does not have 
to {\em imply} the value of the other variables in order for
compression to be possible. It is enough that there is a statistical
dependence between the variables. In other words, that
\[
P(A=a|B=b) \neq P(A=a) \mbox{ and } P(C=c,D=d|B=b) \neq P(C=c,D=d)
\]

The ultimate measure of the compressibility of a relationship is the
{\em entropy} of the relation:
\[
H(P(A,B,C,D))= - \sum_{(a,b,c,d)} P(A=a,B=b,C=c,D=d) \log_2 P(A=a,B=b,C=c,D=d)
\]
According to Shannon's source coding theorem (\cite{}) if a table is
known to have this distribution over it's rows, then it requires
$H(P(A,B,C,D))$ bits of storage per row.

\reminder{Lossy compression TBD}

In order to make use of this compression scheme we need to also store
the {\em model} which is expressed here as $P$. Typically, the models
will not be expressed as a big table with the probability of each
possible combination of attributes. Instead, a parametric
representation of the distribution as a function is typically
used. Such a representation, if it fits the data well, can greatly
reduce the storage and computation resources needed.

\reminder{To be continuted, but give me some feedback!}
}

\eat{
\section{Incremental update of the model}
In a typical application we need to learn the model from the sensor
data that is accumulated over time. For the solution to be practical
we need to be able to incrementally update the model without
re-processing all of the historical data.

We consider two scenarios:
\begin{itemize}
\item {\bf Fixed distribution}
This is the classical situation studied in statistics and machine
learning. For some model families (specifically the exponential
families) there exists compact representations of history called 
{\em sufficient statistics}. Sufficient statistics, such as the
empirical mean and the empirical standard deviation, hold all of the
information needed to evaluate a distribution model wrt the history. 

At the opposite extreme, with complete generality but potentially
prohibitive computational demands, sits Bayesian statistics~\cite{}
and online learning~\cite{Cesa-BianchiLugosi}. Under this approch,
we keep score of the cumulative loss of each model. This approach
provides universal guarantees on the regret - the cumulative loss of
the method is never much worse than the cumulative loss of the best
model in hind-sight.

There is a lot of recent research on methods that lie between these
two extremes - using small summaries of history to achieve performance
that is close to that of the Bayesian methods.

\item {\bf Time-Varying distribution}

\end{itemize}
}
