% Model accuracy definition in modelbaseddbs
% encoder - decoder

\section{Compact Model Representation}
\label{sec:compression}

\reminder{Currently learning algorithms return a model, which is a continuous function. Should they return a compact representation of the function instead?}

As we discussed, models enable query processing on spatiotemporal sensor data. However, models also serve two other important roles: First, being succinct descriptions of the data, they can be represented compactly, leading to significant data compression. This leads not only to decreased storage requirements but also to improved query processing performance, since as we will see in Section \ref{sec:query-processing}, many queries can be evaluated directly on the storage representation. Second, by abstracting out from the specific data values, models can also separate the signal from the noise, leading to higher quality data compared to the original measurements. 

%The practice of data compression~\cite{DBLP:books/mk/Sayood12} shows that most real-world sensor generated sequences can be captured reasonably well using one of a small set of statistical models such as ARMA models, Fourier and wavelet-based models and Singular Value Decomposition models (SVD or PCA). 

Compression, noise and models are closely related. Good models allow \projName\ to achieve high compression ratios. In turn, high compression ratios indicate that \projName\ has correctly identified significant patterns in the data and removed the noise.
We next review the state of the art in compression and describe the compression techniques employed in \projName. 

%The key responsibility of the model administrator (see Figure~\ref{fig-architecture.pdf}) is to choose a set of learning algorithms to create corresponding models over the measurements tables. Each learning algorithm creates a model that fits the data and (in case the administrator has selected to materialize it) stores it using a learning algorithm-specific storage representation. For instance, as shown in Figure~\ref{fig-architecture.pdf}, a possible storage representation of a model created by the ARMA learning algorithm is the set of the ARMA coefficients. Once the chosen models have been created, the administrator can compare them in terms of compression and distortion and choose the one that performs best.

%\reminder{to YF and YK: The term codecs makes below its first appearance.}
%The key responsibility of the model administrator (see Figure~\ref{fig:db-driven-arch}) is to choose a set of viable models, which we refer to as {\em codecs}. Plato will then use machine learning algorithms to tune the parameters of the different codecs so that they best fit the data. The administrator will then compare the performance of the different codecs in terms of compression and distortion and choose the model that performs best.

%\reminder{To YF and YK: There is still a slight terminology mismatch, as below we refer to the storage representation of models, while we really mean the storage representation generated by a particular learning algorithm.}
%There are multiple models and respective compressions that are applicable, as explained next. In the interest of comparisons to prior work, we initiate the discussion with a presentation of lossless compression and then continue with models involving lossy compression. In practice, Plato will use additive and reduced-noise models only. The underlying logic is that we aim for minimum query processing time and high quality models, while storage is a secondary only consideration.

\subsection{State of the art in data compression}

Compression methods are distinguished into lossless and lossy, depending on whether they retain or lose information from the input data, respectively.\\

{\bf Lossless compression.}  A lossless compression method (such as gzip and compress) can accurately reconstruct the original measurements. Looking at it as a statistical model over the data, its expected compression ratio is determined by the distance between the statistical model and the empirical distribution of the data (as measured by the Kullback-Leibler divergence). However, most of the time this compression ratio is around 2 to 4, as the lossless compression in order to accurately capture the entire input, models both the true state of the world (i.e., the signal) and any random influences on the measurements (i.e., the noise).\\


%{\bf Lossless compression.}  A lossless compression method is one which can reconstruct the original measurements without error. Popular lossless compression methods are {\tt gzip} and {\tt compress}. A lossless compression method corresponds to a statistical model over the data and the expected compression ratio is determined by the distance between the statistical model and the empirical distribution of the data (as measured by the the Kullback-Leibler divergence). 

%Unfortunately, lossless compression methods usually achieve a compression ratio of around 2 to 4. This corresponds to the fact that lossless compression accurately captures both the true state of the world (the signal) and the many random influences on the measurements (the noise). Lossless compression does not distinguish between the two.\\

\newcommand{\vx}{\mathbf{x}}
\newcommand{\hx}{\hat{x}}
\newcommand{\vhx}{\hat{\mathbf{x}}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\vr}{\mathbf{r}}

{\bf Lossy compression.} In order to reach higher compression ratios, it is common to use {\em lossy compression}.
Lossy compression is based on the assumption that the input data is a
point in Euclidean space. For instance, let $\vx=(x_1,x_2,$ $\ldots,x_t)$
be a sequence of real values representing the measurements arriving
from a (single) sensor. Similarly, let $\vhx=(\hx_1,\hx_2,\ldots,\hx_t)$
be the reconstruction of the sequence from the compressed version. If the
compression is lossy, the difference $\vr = \vx -\vhx$ (called the \emph{residual})
is non-zero. For a lossy compression to be considered good, the size of the residual,
also known as the \emph{distortion}, should be small. The most common measure of distortion is the $L_2$ error, also called \emph{root-mean-square-error} or \emph{RMS}: $\mbox{RMS}\left(\vhx,\vx \right)
\doteq \sqrt{\frac{\sum_{i=1}^t r_i^2}{t}}$. 
Lossy compression methods such as jpeg2000 for images and
  mp3 for audio can achieve compression ratios of 100 or more
with no perceptible degradation in quality.

%In order to reach higher compression rations than lossless compression, it is common to use {\em lossy compression}. From this point onward we assume that the measurements arriving from a (single) sensor are represented as a sequence of real values $\vx=(x_1,x_2,\ldots,x_t)$. We denote the reconstruction of the sequence from the compressed version by $\vhx=(\hx_1,\hx_2,\ldots,\hx_t)$. As we are using lossy compression $\vhx$ is not equal to $\vx$. The difference between the two sequences is called the {\em residual} which we denote by $\vr = \vx -\vhx$.  We require that the residual is small.  The measure of the size of the residual is called the {\em distortion}. We use two of the most popular distortion measures.

\eat{
\begin{itemize}
\item By far, the most common measure of distortion is the $L_2$ error
  also called {\em root-mean-square} error or RMS:
\[
\mbox{RMS}\left(\vhx,\vx \right)
\doteq \sqrt{\frac{\sum_{i=1}^t r_i^2}{t}}
\]
\item If the errors are usually small and only rarely very large,
the RMS is misleadingly large. A better error measure in such
situations is the fraction of times the error is larger
than some threshold:
\[
\mbox{err}_{\epsilon}\left(\vhx,\vx \right)
\doteq \frac{\sum_{i=1}^t \mathbf{1}\left(|r_i|>\epsilon \right)}{t}
\]
\end{itemize}
}

\reminder{to YF: Instead of audio and image examples, please provide a signal processing case where wavelets/FFT, SVD, ARMA provide good compression. I made a mention to Matt's efforts.}
\reminder{Removed second measure of distortion}

\subsection{Data compression in Plato}

In this spectrum of compression options, \projName\ achieves a novel tradeoff: It achieves a high compression ratio (similar to lossy approaches), while simultaneously retaining sufficient information about the input data (similar to lossless approaches). Moreover, this information is arguably of higher quality than even the original measurements.\\

%In the spectrum between compression with reconstructing ability and compression with low storage requirements, \projName\ achieves a novel tradeoff: It simultaneously achieves high compression ratio and retains sufficient information about the input data that is arguably higher quality than even the original measurements.\\

%Thus there is a tradeoff between lossless compression, which is typically associated with high quality but low compression ratios and lossy compression, which achieves high compression ratios but cannot reconstruct the original signal. \projName\ achieves a new tradeoff in this range of options by employing models that are both lossy, thus leading to high compression ratios and arguably higher quality than lossless compression.\\

{\bf Reduced-noise models.} This is achieved through \emph{reduced-noise models}; i.e., models that separate the signal from the underlying noise. Instead of trying to minimize the amplitude of the residual (measured through the RMS), the learning algorithms employed by \projName\ make sure that the residual is white noise. For linear models one can check whether this is the case by considering the auto-correlation function for each residual and the cross-correlation between each pair of residuals. The residual is considered white noise when the auto-correlation consists of a single delta-function at zero and the cross-correlation functions are close to zero everywhere. Once the learning algorithm has successfully separated the signal from the noise, it creates a model of the form $f(i) = s(i) + \alpha(i) n(i)$, as described in Section \ref{sec:probabilistic-models}, that retains the signal $s$, the description of the noise $n$ (e.g., gaussian) and its amplitude $\alpha$. By losing only the noise, the reduced-noise model is lossy (yielding a high compression ratio), but also retains high quality data (arguably exceeding the quality achieved by lossless models).\\

%{\bf Reduced-noise models.} This is achieved through \emph{reduced-noise models}; i.e., models that separate the signal from the underlying noise. Instead of considering the {\em amplitude} of the residual using RMS or $\mbox{err}_{\epsilon}$ we can consider whether or not the residual is white noise. If the models that we consider are linear we can measure how similar the residual is to white noise by considering the auto-correlation function for each residual and the cross-correlation between each pair of residuals. We expect the auto-correlation to consist of a single delta-function at zero and we expect the cross-correlation functions to be close to zero everywhere. A reduced-noise model is lossy, but the only information that it loses is the noise. In this way, \projName\ manages to get the best of both worlds; it retains the compression ratio of noise models, while preserving the high-quality aspect (and arguably exceeding it) of lossless models.\\

\reminder{Is the noise inference part of the framework or part of each learning algorithm? Do we want to talk about multi-level models?}

\eat{
\subsection{Reduced-noise models}
\label{sec:reduced-noise}

Lossy models invariably loose some of the original
data. However, even with a lossy model it is often the case that the result is a
reconstruction that is {\em closer} to the true underlying real-world
values than the original measurements. 
Noise reduction gives rise to a different way of measuring
the quality of a compression scheme. Instead of considering the {\em
amplitude} of the residual using RMS or $\mbox{err}_{\epsilon}$ we
can consider whether or not the residual is white noise. If the models
that we consider are linear we can measure how similar the residual is
to white noise by considering the auto-correlation function for each
residual and the cross-correlation between each pair of residuals. We
expect the auto-correlation to consist of a single delta-function at
zero and we expect the cross-correlation functions to be close to zero
everywhere.
}

\eat{
\iffalse
This is possible when the model used for compression captures some
inherent regularity in the data. Consider a temperature gauge in a
room that is taking a measurement ten times per second. Suppose also
that each measurement is the sum of the true temperature and gaussian
noise with a standard deviation of one degree.  It is clear that
replacing blocks of 10 consecutive measurements by their average is
lossy in terms of the original signal. However, at the same time, the
averaged sequence is closer to the true measurements. This noise
reduction relies of the assumption that room temperature rarely
changes significantly within a tenth of a second. Therefore any rapid
variations in the measured temperature is likely to be an artifact of
the sensor rather than anything real.
\fi
}

{\bf Improving models and compression by exploiting dependencies.}
The compression ratio can be further improved by building models that cover a set of proximate sensors, behaving similarly. For instance, consider our running example of temperature measurements taken from rooms in office buildings. Since neighboring rooms usually show similar temperature readings, it is beneficial for compression purposes to build a single model for all of them. Unfortunately, the space of models that capture all possible dependencies between sensors is extremely large and hard to learn fully automatically. Therefore, it is the role of the model administrator to specify the set of possible correlations that should be considered by the system to improve compression.\\

\reminder{To YP: IMPORTANT: The way it is described above, we create a single model returning the same value for all similar rooms. This is different from what we had in mind where we get a different value for each room, while using a single core model. Our model definition does not cover this case.}
\reminder{Explain that the model can be shown to the user to understand the correlations that exist in the input data}
\reminder{Change previous examples to mention room temperatures, instead of temperatures in the abstract}


%When the data to be compressed arrives from a large number of proximate sensors, there are potentially large benefits from compressing them together, using a predictive statistical model such as ARMA. Unfortunately, the space of models that capture all possible dependencies between the sensor readings is extremely large and hard to learn fully automatically. In this case, Plato will engage the model administrator in a semiautomatic procedure.

%The role of the model administrator in this context is to restrict the set of possible interactions to make the compression most effective, while keeping the associated the learning problem reasonable easy. In the example of modeling the temperatures of rooms in a building the model might be restricted to considering the dependence between the outside temperature and that of the rooms and in addition, the dependence between neighboring rooms. A model that identifies the stronger dependencies within these restrictions is useful both for data compression, as a model of the physical reality (which rooms are better insulated than others) and as a basis for noise reduction.

%Notice that noise reduction is possible when a good model of the dependencies between different sensors is available. In such situations the compression and decompression of the data shifts the data from the measured vector towards the manifold that is consistent with the physical and statistical properties of the data. The result is similar to that of {\em smoothing} the data, with the important difference that the smoothing is based solely on the statistical model that was inferred from the data.

{\bf Accelerating query processing through additive models.}
Some of the most commonly used models for lossy compression, such as FFT, Wavelets, and SVD are composed of components that can be ordered according to their effect on the RMS measure. We call such models \emph{additive}. Formally:

\begin{defin}
A model is said to be \emph{additive} when it is a sum of components $\vhx =
\sum_{i=1}^k \vc_i$ s.t. the following two conditions hold: (a) the best model for $k_2>k_1$ shares the first $k_1$ components with the best model using $k_1$ components and
(b) the reduction in the distortion is largest for the first component and decreases monotonically as $k$ increases.
\end{defin}

\reminder{What does ``best'' mean?}

Additive models lend themselves to an incremental compression scheme. Starting by setting the residual to the original measurements $\vr \leftarrow \vx$, we can create an additive model by performing the following procedure:\\

\noindent until $\vr$ corresponds to white noise do\\
\noindent\hspace*{0.5cm} Find the vector $\vc$ that minimizes the RMS: 
$\mbox{RMS}\left(\vc,\vr \right)$\\
\noindent\hspace*{0.5cm} Subtract the identified component from the residual:\\
\noindent\hspace*{1cm}$\vr = \vr - \vc$\\

By ordering the model components in decreasing order of importance, the query processing algorithm can perform incremental query answering, producing approximations of the result of ever increasing accuracy, as described next.

\eat{
\subsection{Model Maintenance}
\label{sec:ivm}

When considering a database system that processes data streams over
long periods of time it is necessary to consider how to maintain the
models over time. We divide this problem into two parts, depending on
whether or not the data source is stationary.

When the data source is stationary, efficient model maintenance is
closely related to the subjects of sufficient statistics and or data
synopses or sketches~\cite{DBS-004}. In any of these formulation the goal is
to define a small data structure that stores the information necessary
to define the model. We plan to test the methods proposed in the
literature and, where necessary, develop new ones.

When the data source is not stationary, we need our model to track the
distribution of the data. This causes an unavoidable tradeoff between
accuracy and adaptivity. In order to adapt quickly the model has to
ignore data from the distant past. At the same time, ignoring data
from the distant past limit the accuracy at which the model parameters
can be estimated. Calibrating the algorithm to consider the optimal
window into the past has been extensively studied in online learning
theory~\cite{prediction_learning_models}, in particular
in~\cite{HerbsterWa98,bousquet01tracking,FreundScSiWa97} there is a
detailed study of algorithms for combining models that change over
time. Plato will use these online learning methods for efficient mode
maintenance when the data distribution varies over time.

%
\reminder{skip this subsection. may become irrelevant due to the increasing depth representation discussed in the query processing}
The model administrator can choose more or less compressed model alternatives. The choice presents a speed-accuracy trade-off: Reduced-noise model alternatives will enable precise query answers but at the cost of speed, since their representations will be relatively large. In contrast, 
lossy model alternatives will lead to very fast queries but at the cost of query answer accuracy. Plato will provide a {\em model administrator assistant} module that semiautomates the process of choosing the appropriate model representation by solving the following problems.

% query answering using a model alternative
% what is the accuracy damage

\paragraph{Choosing a model alternative given a query}
In the simplest setting, the assistant is given
\begin{enumerate} 
%
\item data measurements.
%
\item a model generator that can produce a reduced-noise model $f_{nr}$ of the measurements
%
\item a lossy model generator and candidate loss parameters. For example, the assistant may given the \texttt{fourier\_rms} model generator with candidate RMS error (loss parameter) 0.01, 0.02, 0.03, ..., 0.20. In principle, each setting $e_i$ of the loss parameter leads to another model $f_i$. Furthermore, the representation size of $f_i$ is larger than the representation size of $f_{i+1}$ and all of them are smaller than $f_{nr}$. Let us call 
%
\item a single query $Q$ that uses a model $f$ and a specification of the desired accuracy of the result.
%
\end{enumerate}
}
